{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "import tensorflow as tf\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg') # Must be before importing matplotlib.pyplot or pylab!\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from fast_rcnn.config import cfg\n",
    "from fast_rcnn.test import im_detect\n",
    "from fast_rcnn.nms_wrapper import nms\n",
    "from utils.timer import Timer\n",
    "import numpy as np\n",
    "import os, sys, cv2\n",
    "import argparse\n",
    "from networks.factory import get_network\n",
    "\n",
    "CLASSES =('__background__', 'arm')\n",
    "\n",
    "# CLASSES =('__background__', \n",
    "#                             \"box\",\n",
    "#                             \"gum\",\n",
    "#                             \"marker\",\n",
    "#                             \"pen\",\n",
    "#                             \"postit\",\n",
    "#                             \"scissors\",\n",
    "#                             \"tape\",\n",
    "#                             \"usb\"\n",
    "#                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#CLASSES = ('__background__','person','bike','motorbike','car','bus')\n",
    "\n",
    "def vis_detections(im, class_name, dets,ax, thresh=0.5):\n",
    "    \"\"\"Draw detected bounding boxes.\"\"\"\n",
    "    inds = np.where(dets[:, -1] >= thresh)[0]\n",
    "    if len(inds) == 0:\n",
    "        return\n",
    "\n",
    "    for i in inds:\n",
    "        bbox = dets[i, :4]\n",
    "        score = dets[i, -1]\n",
    "\n",
    "           ax.text(bbox[0], bbox[1] - 2,\n",
    "                '{:s} {:.3f}'.format(class_name, score),\n",
    "                bbox=dict(facecolor='blue', alpha=0.5),\n",
    "                fontsize=14, color='white')\n",
    "\n",
    "    ax.set_title(('{} detections with '\n",
    "                  'p({} | box) >= {:.1f}').format(class_name, class_name,\n",
    "                                                  thresh),\n",
    "                  fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def demo(sess, net, image_name):\n",
    "    \"\"\"Detect object classes in an image using pre-computed object proposals.\"\"\"\n",
    "\n",
    "    # Load the demo image\n",
    "    im_file = os.path.join(cfg.DATA_DIR, 'office_supplies', 'images', image_name)\n",
    "    #im_file = os.path.join('/home/corgi/Lab/label/pos_frame/ACCV/training/000001/',image_name)\n",
    "    im = cv2.imread(im_file)\n",
    "\n",
    "    # Detect all object classes and regress object bounds\n",
    "    timer = Timer()\n",
    "    timer.tic()\n",
    "    scores, boxes = im_detect(sess, net, im)\n",
    "    timer.toc()\n",
    "    print ('Detection took {:.3f}s for '\n",
    "           '{:d} object proposals').format(timer.total_time, boxes.shape[0])\n",
    "\n",
    "    # Visualize detections for each class\n",
    "    im = im[:, :, (2, 1, 0)]\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.imshow(im, aspect='equal')\n",
    "\n",
    "    CONF_THRESH = 0.15\n",
    "    NMS_THRESH = 0.08\n",
    "    for cls_ind, cls in enumerate(CLASSES[1:]):\n",
    "        cls_ind += 1 # because we skipped background\n",
    "        cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]\n",
    "        cls_scores = scores[:, cls_ind]\n",
    "        dets = np.hstack((cls_boxes,\n",
    "                          cls_scores[:, np.newaxis])).astype(np.float32)\n",
    "        keep = nms(dets, NMS_THRESH)\n",
    "        dets = dets[keep, :]\n",
    "        vis_detections(im, cls, dets, ax, thresh=CONF_THRESH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfg.TEST.HAS_RPN = True  # Use RPN for proposals\n",
    "gpu_id = 1\n",
    "demo_net = \"VGGnet_test\"\n",
    "# model = \"/data/code/Faster-RCNN_TF/output/faster_rcnn_end2end/office_supplies/VGGnet_fast_rcnn_iter_200.ckpt\"\n",
    "model = \"/data/code/Faster-RCNN_TF/output/faster_rcnn_end2end/armpos/VGGnet_fast_rcnn_iter_2000.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfg.DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image_name = 'pedestrian_cars.jpg'\n",
    "# im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name)\n",
    "im = cv2.imread(\"/data/code/Faster-RCNN_TF/data/armpos/images/10.jpg\")\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = get_network(demo_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, model)\n",
    "\n",
    "print('\\n\\nLoaded network {:s}'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('cls_score', reuse=True):\n",
    "    print(tf.get_variable('weights').get_shape())\n",
    "    print(tf.get_variable('biases').get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = net.layers\n",
    "# out_layers = {'cls_score', 'bbox_pred', 'roi-data'}\n",
    "out_layers = {}\n",
    "out_dict = {}\n",
    "for layer in layers:\n",
    "    if layer in out_layers:\n",
    "        continue\n",
    "    allvsrs = tf.get_collection(tf.GraphKeys.VARIABLES, layer)\n",
    "    if len(allvsrs) > 0:\n",
    "        d={}\n",
    "        for variable in allvsrs:\n",
    "            d[variable.name.split('/')[1].split(':')[0]] = variable.eval(session=sess)\n",
    "        out_dict[layer] = d\n",
    "# with tf.variable_scope(, reuse=True):\n",
    "#     td = tf.get_variable('weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = ['obj', 'arm']\n",
    "excepts = {'conv1_1','conv1_2', 'conv2_1', 'conv1_1'}\n",
    "def split_model(model_dict, splits, excepts):\n",
    "    split_dict = {}\n",
    "    split_maps = [(key, [split + '_' + key for split in splits]) \n",
    "     if key not in excepts \n",
    "     else (key, [key])  for key in model_dict.keys()]\n",
    "    for (inky, outkys) in split_maps:\n",
    "        for outky in outkys:\n",
    "            split_dict[outky] = model_dict[inky]\n",
    "    return split_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_dict = split_model(out_dict, splits, excepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/data/tmp/split_trained_model.npy', 'w') as f:\n",
    "    np.save(f, split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_dict['arm_fc7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "im_names = ['11.jpg', '33.jpg', '44.jpg', '55.jpg', '66.jpg']\n",
    "#im_names = ['000456.jpg']\n",
    "for im_name in im_names:\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Demo for data/demo/{}'.format(im_name))\n",
    "    demo(sess, net, im_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/data/robotics/raw_position_images/train_2' + '.tfrecords'\n",
    "\n",
    "example = tf.train.Example()\n",
    "pi = tf.python_io.tf_record_iterator(filename)\n",
    "example.ParseFromString(pi.next())\n",
    "\n",
    "dat = np.fromstring(example.features.feature[\"left_image\"].bytes_list.value[0], dtype=np.uint8)\n",
    "\n",
    "\n",
    "im = dat.reshape((480, 640, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(enumerate([1,2][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores, boxes = im_detect(sess, net, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# im = im[:, :, (2, 1, 0)]\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.imshow(im, aspect='equal')\n",
    "\n",
    "CONF_THRESH = 0.04\n",
    "NMS_THRESH = 0.03\n",
    "for cls_ind, cls in enumerate(CLASSES[1:]):\n",
    "    cls_ind += 1 # because we skipped background\n",
    "    print cls_ind\n",
    "    print scores.shape\n",
    "    cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]\n",
    "    cls_scores = scores[:, cls_ind]\n",
    "    dets = np.hstack((cls_boxes,\n",
    "                      cls_scores[:, np.newaxis])).astype(np.float32)\n",
    "    keep = nms(dets, NMS_THRESH)\n",
    "    dets = dets[keep, :]\n",
    "    print(keep)\n",
    "    vis_detections(im, cls, dets, ax, thresh=CONF_THRESH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dets = np.hstack((cls_boxes,\n",
    "                  cls_scores[:, np.newaxis])).astype(np.float32)\n",
    "keep = nms(dets, NMS_THRESH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.where(dets[:, -1] >= 0.04)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dets[keep, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls_ind = 2\n",
    "boxes[:, 4*cls_ind:4*(cls_ind + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.imshow(im)\n",
    "fig.canvas.mpl_connect( \"button_press_event\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores, boxes = im_detect(sess, net, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(scores), len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.hstack([scores, boxes])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x0, y0, x1, y_1) = linebuilder.x_0, linebuilder.y_0, linebuilder.x_press, linebuilder.y_press\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
